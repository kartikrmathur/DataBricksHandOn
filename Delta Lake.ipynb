{"cells":[{"cell_type":"markdown","source":["<!-- You can run this notebook in a Databricks environment. Specifically, this notebook has been designed to run in [Databricks Community Edition](http://community.cloud.databricks.com/) as well. -->\nTo run this notebook, you have to [create a cluster](https://docs.databricks.com/clusters/create.html) with version **Databricks Runtime 7.4 or later** and [attach this notebook](https://docs.databricks.com/notebooks/notebooks-manage.html#attach-a-notebook-to-a-cluster) to that cluster. <br/>\n\n### Source Data for this notebook\nThe data used is a modified version of the public data from [Lending Club](https://www.kaggle.com/wendykan/lending-club-loan-data). It includes all funded loans from 2012 to 2017. Each loan includes applicant information provided by the applicant as well as the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. For a full view of the data please view the data dictionary available [here](https://resources.lendingclub.com/LCDataDictionary.xlsx)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d7056c3-48bd-403b-a68b-520d25b5031c"}}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91975e6f-c001-4758-8b58-965768a0c0e7"}}},{"cell_type":"code","source":["db = \"deltadb\"\n\nspark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\nspark.sql(f\"USE {db}\")\n\nspark.sql(\"SET spark.databricks.delta.formatCheck.enabled = false\")\nspark.sql(\"SET spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0e19967-fa72-488b-bc95-eff206c6079e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: DataFrame[key: string, value: string]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: DataFrame[key: string, value: string]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import random\nfrom datetime import datetime\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n\ndef my_checkpoint_dir(): \n  return \"/tmp/delta_demo/chkpt/%s\" % str(random.randint(0, 10000))\n\n# User-defined function to generate random state\n@udf(returnType=StringType())\ndef random_state():\n  return str(random.choice([\"CA\", \"TX\", \"NY\", \"WA\"]))\n\n\n# Function to start a streaming query with a stream of randomly generated load data and append to the parquet table\ndef generate_and_append_data_stream(table_format, table_name, schema_ok=False, type=\"batch\"):\n  \n  stream_data = (spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 500).load()\n    .withColumn(\"loan_id\", 10000 + col(\"value\"))\n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\"))\n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000))\n    .withColumn(\"addr_state\", random_state())\n    .withColumn(\"type\", lit(type)))\n    \n  if schema_ok:\n    stream_data = stream_data.select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\", \"type\", \"timestamp\")\n      \n  query = (stream_data.writeStream\n    .format(table_format)\n    .option(\"checkpointLocation\", my_checkpoint_dir())\n    .trigger(processingTime = \"5 seconds\")\n    .table(table_name))\n\n  return query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7dfa2b49-73ba-4ec1-bc3f-34b9c0bc8276"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Function to stop all streaming queries \ndef stop_all_streams():\n    print(\"Stopping all streams\")\n    for s in spark.streams.active:\n        try:\n            s.stop()\n        except:\n            pass\n    print(\"Stopped all streams\")\n    dbutils.fs.rm(\"/tmp/delta_demo/chkpt/\", True)\n\n\ndef cleanup_paths_and_tables():\n    dbutils.fs.rm(\"/tmp/delta_demo/\", True)\n    dbutils.fs.rm(\"file:/dbfs/tmp/delta_demo/loans_parquet/\", True)\n        \n    for table in [\"deltadb.loans_parquet\", \"deltadb.loans_delta\", \"deltadb.loans_delta2\"]:\n        spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n    \ncleanup_paths_and_tables()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5173e110-562a-4da9-896b-913073795561"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%sh mkdir -p /dbfs/tmp/delta_demo/loans_parquet/; wget -O /dbfs/tmp/delta_demo/loans_parquet/loans.parquet https://pages.databricks.com/rs/094-YMS-629/images/SAISEU19-loan-risks.snappy.parquet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54ee6c98-302b-4ca5-833b-34bdb9cbaa4a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">--2021-12-09 15:38:31--  https://pages.databricks.com/rs/094-YMS-629/images/SAISEU19-loan-risks.snappy.parquet\nResolving pages.databricks.com (pages.databricks.com)... 104.17.71.206, 104.17.72.206, 104.17.73.206, ...\nConnecting to pages.databricks.com (pages.databricks.com)|104.17.71.206|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 164631 (161K) [text/plain]\nSaving to: ‘/dbfs/tmp/delta_demo/loans_parquet/loans.parquet’\n\n     0K .......... .......... .......... .......... .......... 31% 4.53M 0s\n    50K .......... .......... .......... .......... .......... 62% 6.41M 0s\n   100K .......... .......... .......... .......... .......... 93% 6.89M 0s\n   150K ..........                                            100% 33.3M=0.03s\n\n2021-12-09 15:38:31 (6.09 MB/s) - ‘/dbfs/tmp/delta_demo/loans_parquet/loans.parquet’ saved [164631/164631]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">--2021-12-09 15:38:31--  https://pages.databricks.com/rs/094-YMS-629/images/SAISEU19-loan-risks.snappy.parquet\nResolving pages.databricks.com (pages.databricks.com)... 104.17.71.206, 104.17.72.206, 104.17.73.206, ...\nConnecting to pages.databricks.com (pages.databricks.com)|104.17.71.206|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 164631 (161K) [text/plain]\nSaving to: ‘/dbfs/tmp/delta_demo/loans_parquet/loans.parquet’\n\n     0K .......... .......... .......... .......... .......... 31% 4.53M 0s\n    50K .......... .......... .......... .......... .......... 62% 6.41M 0s\n   100K .......... .......... .......... .......... .......... 93% 6.89M 0s\n   150K ..........                                            100% 33.3M=0.03s\n\n2021-12-09 15:38:31 (6.09 MB/s) - ‘/dbfs/tmp/delta_demo/loans_parquet/loans.parquet’ saved [164631/164631]\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Getting started with <img src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\" width=300/>\n\nAn open-source storage layer for data lakes that brings ACID transactions to Apache Spark™ and big data workloads.\n\n* **ACID Transactions**: Ensures data integrity and read consistency with complex, concurrent data pipelines.\n* **Unified Batch and Streaming Source and Sink**: A table in Delta Lake is both a batch table, as well as a streaming source and sink. Streaming data ingest, batch historic backfill, and interactive queries all just work out of the box. \n* **Schema Enforcement and Evolution**: Ensures data cleanliness by blocking writes with unexpected.\n* **Time Travel**: Query previous versions of the table by time or version number.\n* **Deletes and upserts**: Supports deleting and upserting into tables with programmatic APIs.\n* **Open Format**: Stored as Parquet format in blob storage.\n* **Audit History**: History of all the operations that happened in the table.\n* **Scalable Metadata management**: Able to handle millions of files are scaling the metadata operations with Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"868e575b-aadd-4e91-a4dc-027c3a5ee20d"}}},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Convert to Delta Lake format"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f2ecb50-1232-4b55-a8dc-0d28cffe23b6"}}},{"cell_type":"markdown","source":["Delta Lake is 100% compatible with Apache Spark&trade;, which makes it easy to get started with if you already use Spark for your big data workflows.\nDelta Lake features APIs for **SQL**, **Python**, and **Scala**, so that you can use it in whatever language you feel most comfortable in."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07c927e4-6da4-4a85-b254-09a78b5f2855"}}},{"cell_type":"markdown","source":["<img src=\"https://databricks.com/wp-content/uploads/2020/12/simplysaydelta.png\" width=600/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bc774f7-3a74-41af-aa68-451d614094ae"}}},{"cell_type":"markdown","source":["In **Python**: Read your data into a Spark DataFrame, then write it out in Delta Lake format directly, with no upfront schema definition needed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"240a3e85-6916-42d3-b523-79b57f56044e"}}},{"cell_type":"code","source":["parquet_path = \"file:/dbfs/tmp/delta_demo/loans_parquet/\"\n\ndf = (spark.read.format(\"parquet\").load(parquet_path)\n      .withColumn(\"type\", lit(\"batch\"))\n      .withColumn(\"timestamp\", current_timestamp()))\n\ndf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"loans_delta\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2bb8bcc-1f21-4a8b-8538-79acb1623b31"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"loan_id","nullable":true,"type":"long"},{"metadata":{},"name":"funded_amnt","nullable":true,"type":"integer"},{"metadata":{},"name":"paid_amnt","nullable":true,"type":"double"},{"metadata":{},"name":"addr_state","nullable":true,"type":"string"},{"metadata":{},"name":"type","nullable":false,"type":"string"},{"metadata":{},"name":"timestamp","nullable":false,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["**SQL:** Use `CREATE TABLE` statement with SQL (no upfront schema definition needed)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72b4e700-d9ea-4782-9f73-c6901928bdc1"}}},{"cell_type":"code","source":["%sql\nCREATE TABLE loans_delta2\nUSING delta\nAS SELECT * FROM parquet.`/tmp/delta_demo/loans_parquet`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c7c9822-3045-4926-a0c7-33fc25565d25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Path does not exist: dbfs:/tmp/delta_demo/loans_parquet;; line 3 pos 17\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.org$apache$spark$sql$execution$datasources$ResolveSQLOnFile$$resolveDataSource(rules.scala:118)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:142)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:216)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:414)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:412)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:365)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:216)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:414)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:412)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:365)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:216)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:122)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:46)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:152)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:149)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:141)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:217)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:180)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:119)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:119)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:200)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:91)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:171)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:81)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.AnalysisException: Path does not exist: dbfs:/tmp/delta_demo/loans_parquet;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:827)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:810)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:618)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.org$apache$spark$sql$execution$datasources$ResolveSQLOnFile$$resolveDataSource(rules.scala:113)\n\t... 93 more\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: AnalysisException: Path does not exist: dbfs:/tmp/delta_demo/loans_parquet;; line 3 pos 17","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Path does not exist: dbfs:/tmp/delta_demo/loans_parquet;; line 3 pos 17\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.org$apache$spark$sql$execution$datasources$ResolveSQLOnFile$$resolveDataSource(rules.scala:118)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:142)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:216)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:414)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:412)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:365)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:216)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:414)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:412)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:365)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:216)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:122)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:46)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:152)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:149)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:141)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:217)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:180)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:119)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:119)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:200)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:91)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:171)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:81)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.AnalysisException: Path does not exist: dbfs:/tmp/delta_demo/loans_parquet;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:827)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:810)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:618)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.org$apache$spark$sql$execution$datasources$ResolveSQLOnFile$$resolveDataSource(rules.scala:113)\n\t... 93 more\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["**SQL**: Use `CONVERT TO DELTA` to convert Parquet files to Delta Lake format in place"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dff00b6-fe5a-4b1a-af7e-32ae4c2c8875"}}},{"cell_type":"code","source":["%sql CONVERT TO DELTA parquet.`/tmp/delta_demo/loans_parquet`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08d2314b-176b-4ddf-8709-1153a71f2fe4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: java.io.FileNotFoundException: No file found in the directory: dbfs:/tmp/delta_demo/loans_parquet.\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.emptyDirectoryException(DeltaErrors.scala:967)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.$anonfun$performConvert$1(ConvertToDeltaCommand.scala:338)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:19)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:19)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:19)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:56)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:129)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:71)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:58)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:85)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:401)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:380)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.recordOperation(ConvertToDeltaCommand.scala:76)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:108)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:94)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.recordDeltaOperation(ConvertToDeltaCommand.scala:76)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.performConvert(ConvertToDeltaCommand.scala:321)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.$anonfun$run$1(ConvertToDeltaCommand.scala:113)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1128)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.run(ConvertToDeltaCommand.scala:85)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:234)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3709)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3707)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:234)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: FileNotFoundException: No file found in the directory: dbfs:/tmp/delta_demo/loans_parquet.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: java.io.FileNotFoundException: No file found in the directory: dbfs:/tmp/delta_demo/loans_parquet.\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.emptyDirectoryException(DeltaErrors.scala:967)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.$anonfun$performConvert$1(ConvertToDeltaCommand.scala:338)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:19)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:19)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:338)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:19)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:56)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:129)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:71)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:58)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:85)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:401)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:380)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.recordOperation(ConvertToDeltaCommand.scala:76)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:108)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:94)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.recordDeltaOperation(ConvertToDeltaCommand.scala:76)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.performConvert(ConvertToDeltaCommand.scala:321)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.$anonfun$run$1(ConvertToDeltaCommand.scala:113)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1128)\n\tat com.databricks.sql.transaction.tahoe.commands.ConvertToDeltaCommandBase.run(ConvertToDeltaCommand.scala:85)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:234)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3709)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3707)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:234)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### View the data in the Delta Lake table\n**How many records are there, and what does the data look like?**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b07485f8-6041-4b63-a63f-a35be2fa14c3"}}},{"cell_type":"code","source":["spark.sql(\"select count(*) from loans_delta\").show()\nspark.sql(\"select * from loans_delta\").show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2644fa0-5841-44fc-89e3-a1072e79f586"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+\n|count(1)|\n+--------+\n|   14705|\n+--------+\n\n+-------+-----------+---------+----------+-----+--------------------+\n|loan_id|funded_amnt|paid_amnt|addr_state| type|           timestamp|\n+-------+-----------+---------+----------+-----+--------------------+\n|      0|       1000|   182.22|        CA|batch|2021-12-09 15:38:...|\n|      1|       1000|   361.19|        WA|batch|2021-12-09 15:38:...|\n|      2|       1000|   176.26|        TX|batch|2021-12-09 15:38:...|\n+-------+-----------+---------+----------+-----+--------------------+\nonly showing top 3 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ncount(1)|\n+--------+\n   14705|\n+--------+\n\n+-------+-----------+---------+----------+-----+--------------------+\nloan_id|funded_amnt|paid_amnt|addr_state| type|           timestamp|\n+-------+-----------+---------+----------+-----+--------------------+\n      0|       1000|   182.22|        CA|batch|2021-12-09 15:38:...|\n      1|       1000|   361.19|        WA|batch|2021-12-09 15:38:...|\n      2|       1000|   176.26|        TX|batch|2021-12-09 15:38:...|\n+-------+-----------+---------+----------+-----+--------------------+\nonly showing top 3 rows\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Unified batch + streaming data processing with multiple concurrent readers and writers"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d533049-f82c-4a76-9734-b3308fbe51f0"}}},{"cell_type":"markdown","source":["### Write 2 different data streams into our Delta Lake table at the same time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e6213a8-f44e-44bd-95f1-0f45a927f21f"}}},{"cell_type":"code","source":["# Set up 2 streaming writes to our table\nstream_query_A = generate_and_append_data_stream(table_format=\"delta\", table_name=\"loans_delta\", schema_ok=True, type='stream A')\nstream_query_B = generate_and_append_data_stream(table_format=\"delta\", table_name=\"loans_delta\", schema_ok=True, type='stream B')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ce2ac1b-f8f5-4ae6-aa58-96fc74d27643"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Create 2 continuous streaming readers of our Delta Lake table to illustrate streaming progress."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3488b36-e42c-475f-b23e-1dd14bdc369a"}}},{"cell_type":"code","source":["# Streaming read #1\ndisplay(spark.readStream.format(\"delta\").table(\"loans_delta\").groupBy(\"type\").count().orderBy(\"type\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4dc4faec-0a21-4486-ab98-f06de53b90ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["batch",14705],["stream A",167000],["stream B",88500]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{"plotlyBar":[{"key":"grouped","value":false},{"key":"stacked","value":true},{"key":"100_stacked","value":false}]},"pivotColumns":["type"],"pivotAggregation":"sum","xColumns":["type"],"yColumns":["count"]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"type","type":"\"string\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>type</th><th>count</th></tr></thead><tbody><tr><td>batch</td><td>14705</td></tr><tr><td>stream A</td><td>167000</td></tr><tr><td>stream B</td><td>88500</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Streaming read #2\ndisplay(spark.readStream.format(\"delta\").table(\"loans_delta\").groupBy(\"type\", window(\"timestamp\", \"10 seconds\")).count().orderBy(\"window\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2463b57-4329-41d5-9def-2a27e121a7a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["batch",["2021-12-09T15:38:30.000+0000","2021-12-09T15:38:40.000+0000"],14705],["stream A",["2021-12-09T15:40:20.000+0000","2021-12-09T15:40:30.000+0000"],3637],["stream B",["2021-12-09T15:40:20.000+0000","2021-12-09T15:40:30.000+0000"],2835],["stream A",["2021-12-09T15:40:30.000+0000","2021-12-09T15:40:40.000+0000"],5000],["stream B",["2021-12-09T15:40:30.000+0000","2021-12-09T15:40:40.000+0000"],5000],["stream A",["2021-12-09T15:40:40.000+0000","2021-12-09T15:40:50.000+0000"],5000],["stream B",["2021-12-09T15:40:40.000+0000","2021-12-09T15:40:50.000+0000"],5000],["stream B",["2021-12-09T15:40:50.000+0000","2021-12-09T15:41:00.000+0000"],5000],["stream A",["2021-12-09T15:40:50.000+0000","2021-12-09T15:41:00.000+0000"],5000],["stream A",["2021-12-09T15:41:00.000+0000","2021-12-09T15:41:10.000+0000"],5000],["stream B",["2021-12-09T15:41:00.000+0000","2021-12-09T15:41:10.000+0000"],5000],["stream B",["2021-12-09T15:41:10.000+0000","2021-12-09T15:41:20.000+0000"],5000],["stream A",["2021-12-09T15:41:10.000+0000","2021-12-09T15:41:20.000+0000"],5000],["stream B",["2021-12-09T15:41:20.000+0000","2021-12-09T15:41:30.000+0000"],5000],["stream A",["2021-12-09T15:41:20.000+0000","2021-12-09T15:41:30.000+0000"],5000],["stream B",["2021-12-09T15:41:30.000+0000","2021-12-09T15:41:40.000+0000"],5000],["stream A",["2021-12-09T15:41:30.000+0000","2021-12-09T15:41:40.000+0000"],5000],["stream A",["2021-12-09T15:41:40.000+0000","2021-12-09T15:41:50.000+0000"],5000],["stream B",["2021-12-09T15:41:40.000+0000","2021-12-09T15:41:50.000+0000"],5000],["stream A",["2021-12-09T15:41:50.000+0000","2021-12-09T15:42:00.000+0000"],5000],["stream B",["2021-12-09T15:41:50.000+0000","2021-12-09T15:42:00.000+0000"],5000],["stream A",["2021-12-09T15:42:00.000+0000","2021-12-09T15:42:10.000+0000"],5000],["stream B",["2021-12-09T15:42:00.000+0000","2021-12-09T15:42:10.000+0000"],5000],["stream A",["2021-12-09T15:42:10.000+0000","2021-12-09T15:42:20.000+0000"],5000],["stream B",["2021-12-09T15:42:10.000+0000","2021-12-09T15:42:20.000+0000"],5000],["stream A",["2021-12-09T15:42:20.000+0000","2021-12-09T15:42:30.000+0000"],5000],["stream B",["2021-12-09T15:42:20.000+0000","2021-12-09T15:42:30.000+0000"],5000],["stream B",["2021-12-09T15:42:30.000+0000","2021-12-09T15:42:40.000+0000"],5000],["stream A",["2021-12-09T15:42:30.000+0000","2021-12-09T15:42:40.000+0000"],5000],["stream A",["2021-12-09T15:42:40.000+0000","2021-12-09T15:42:50.000+0000"],5000],["stream B",["2021-12-09T15:42:40.000+0000","2021-12-09T15:42:50.000+0000"],5000],["stream A",["2021-12-09T15:42:50.000+0000","2021-12-09T15:43:00.000+0000"],5000],["stream B",["2021-12-09T15:42:50.000+0000","2021-12-09T15:43:00.000+0000"],5000],["stream A",["2021-12-09T15:43:00.000+0000","2021-12-09T15:43:10.000+0000"],5000],["stream B",["2021-12-09T15:43:00.000+0000","2021-12-09T15:43:10.000+0000"],5000],["stream B",["2021-12-09T15:43:10.000+0000","2021-12-09T15:43:20.000+0000"],5000],["stream A",["2021-12-09T15:43:10.000+0000","2021-12-09T15:43:20.000+0000"],5000],["stream A",["2021-12-09T15:43:20.000+0000","2021-12-09T15:43:30.000+0000"],5000],["stream B",["2021-12-09T15:43:20.000+0000","2021-12-09T15:43:30.000+0000"],665],["stream A",["2021-12-09T15:43:30.000+0000","2021-12-09T15:43:40.000+0000"],5000],["stream A",["2021-12-09T15:43:40.000+0000","2021-12-09T15:43:50.000+0000"],5000],["stream A",["2021-12-09T15:43:50.000+0000","2021-12-09T15:44:00.000+0000"],3863]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{"plotlyBar":[{"key":"grouped","value":false},{"key":"stacked","value":true},{"key":"100_stacked","value":false}]},"pivotColumns":["type"],"pivotAggregation":"sum","xColumns":["window"],"yColumns":["count"]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"type","type":"\"string\"","metadata":"{}"},{"name":"window","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"start\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"end\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>type</th><th>window</th><th>count</th></tr></thead><tbody><tr><td>batch</td><td>List(2021-12-09T15:38:30.000+0000, 2021-12-09T15:38:40.000+0000)</td><td>14705</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:40:20.000+0000, 2021-12-09T15:40:30.000+0000)</td><td>3637</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:40:20.000+0000, 2021-12-09T15:40:30.000+0000)</td><td>2835</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:40:30.000+0000, 2021-12-09T15:40:40.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:40:30.000+0000, 2021-12-09T15:40:40.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:40:40.000+0000, 2021-12-09T15:40:50.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:40:40.000+0000, 2021-12-09T15:40:50.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:40:50.000+0000, 2021-12-09T15:41:00.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:40:50.000+0000, 2021-12-09T15:41:00.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:41:00.000+0000, 2021-12-09T15:41:10.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:41:00.000+0000, 2021-12-09T15:41:10.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:41:10.000+0000, 2021-12-09T15:41:20.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:41:10.000+0000, 2021-12-09T15:41:20.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:41:20.000+0000, 2021-12-09T15:41:30.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:41:20.000+0000, 2021-12-09T15:41:30.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:41:30.000+0000, 2021-12-09T15:41:40.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:41:30.000+0000, 2021-12-09T15:41:40.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:41:40.000+0000, 2021-12-09T15:41:50.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:41:40.000+0000, 2021-12-09T15:41:50.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:41:50.000+0000, 2021-12-09T15:42:00.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:41:50.000+0000, 2021-12-09T15:42:00.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:42:00.000+0000, 2021-12-09T15:42:10.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:42:00.000+0000, 2021-12-09T15:42:10.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:42:10.000+0000, 2021-12-09T15:42:20.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:42:10.000+0000, 2021-12-09T15:42:20.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:42:20.000+0000, 2021-12-09T15:42:30.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:42:20.000+0000, 2021-12-09T15:42:30.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:42:30.000+0000, 2021-12-09T15:42:40.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:42:30.000+0000, 2021-12-09T15:42:40.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:42:40.000+0000, 2021-12-09T15:42:50.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:42:40.000+0000, 2021-12-09T15:42:50.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:42:50.000+0000, 2021-12-09T15:43:00.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:42:50.000+0000, 2021-12-09T15:43:00.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:43:00.000+0000, 2021-12-09T15:43:10.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:43:00.000+0000, 2021-12-09T15:43:10.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:43:10.000+0000, 2021-12-09T15:43:20.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:43:10.000+0000, 2021-12-09T15:43:20.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:43:20.000+0000, 2021-12-09T15:43:30.000+0000)</td><td>5000</td></tr><tr><td>stream B</td><td>List(2021-12-09T15:43:20.000+0000, 2021-12-09T15:43:30.000+0000)</td><td>665</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:43:30.000+0000, 2021-12-09T15:43:40.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:43:40.000+0000, 2021-12-09T15:43:50.000+0000)</td><td>5000</td></tr><tr><td>stream A</td><td>List(2021-12-09T15:43:50.000+0000, 2021-12-09T15:44:00.000+0000)</td><td>3863</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Add a batch query, just for good measure"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68a01069-6770-41d7-bf5f-f7ce40020821"}}},{"cell_type":"code","source":["%sql\nSELECT addr_state, COUNT(*)\nFROM loans_delta\nGROUP BY addr_state"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2a8fe1c-efef-44af-b61a-1025057867c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["AZ",329],["SC",174],["LA",167],["MN",256],["NJ",541],["DC",38],["OR",178],["VA",413],["RI",66],["WY",31],["KY",157],["NH",68],["MI",366],["NV",219],["WI",202],["ID",15],["CA",80223],["CT",216],["NE",41],["MT",28],["NC",398],["VT",32],["MD",370],["DE",31],["MO",223],["IL",578],["ME",15],["WA",78973],["ND",20],["MS",84],["AL",183],["IN",244],["OH",504],["TN",223],["NM",68],["PA",544],["SD",31],["NY",80485],["TX",79251],["WV",60],["GA",495],["MA",311],["KS",118],["FL",1005],["CO",288],["AK",35],["AR",110],["OK",146],["UT",76],["HI",76]],"plotOptions":{"displayType":"mapPlot","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"addr_state","type":"\"string\"","metadata":"{}"},{"name":"count(1)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>addr_state</th><th>count(1)</th></tr></thead><tbody><tr><td>AZ</td><td>329</td></tr><tr><td>SC</td><td>174</td></tr><tr><td>LA</td><td>167</td></tr><tr><td>MN</td><td>256</td></tr><tr><td>NJ</td><td>541</td></tr><tr><td>DC</td><td>38</td></tr><tr><td>OR</td><td>178</td></tr><tr><td>VA</td><td>413</td></tr><tr><td>RI</td><td>66</td></tr><tr><td>WY</td><td>31</td></tr><tr><td>KY</td><td>157</td></tr><tr><td>NH</td><td>68</td></tr><tr><td>MI</td><td>366</td></tr><tr><td>NV</td><td>219</td></tr><tr><td>WI</td><td>202</td></tr><tr><td>ID</td><td>15</td></tr><tr><td>CA</td><td>80223</td></tr><tr><td>CT</td><td>216</td></tr><tr><td>NE</td><td>41</td></tr><tr><td>MT</td><td>28</td></tr><tr><td>NC</td><td>398</td></tr><tr><td>VT</td><td>32</td></tr><tr><td>MD</td><td>370</td></tr><tr><td>DE</td><td>31</td></tr><tr><td>MO</td><td>223</td></tr><tr><td>IL</td><td>578</td></tr><tr><td>ME</td><td>15</td></tr><tr><td>WA</td><td>78973</td></tr><tr><td>ND</td><td>20</td></tr><tr><td>MS</td><td>84</td></tr><tr><td>AL</td><td>183</td></tr><tr><td>IN</td><td>244</td></tr><tr><td>OH</td><td>504</td></tr><tr><td>TN</td><td>223</td></tr><tr><td>NM</td><td>68</td></tr><tr><td>PA</td><td>544</td></tr><tr><td>SD</td><td>31</td></tr><tr><td>NY</td><td>80485</td></tr><tr><td>TX</td><td>79251</td></tr><tr><td>WV</td><td>60</td></tr><tr><td>GA</td><td>495</td></tr><tr><td>MA</td><td>311</td></tr><tr><td>KS</td><td>118</td></tr><tr><td>FL</td><td>1005</td></tr><tr><td>CO</td><td>288</td></tr><tr><td>AK</td><td>35</td></tr><tr><td>AR</td><td>110</td></tr><tr><td>OK</td><td>146</td></tr><tr><td>UT</td><td>76</td></tr><tr><td>HI</td><td>76</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(\"stop\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58db7257-9bc7-4c39-b3f1-e58ee37114c3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"exit","data":"stop","arguments":{},"metadata":{}}},"output_type":"display_data","data":{"text/plain":["stop"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f69b75b-7ef5-4b58-a5e1-765af60672a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Stopping all streams\nStopped all streams\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Stopping all streams\nStopped all streams\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) ACID Transactions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"257cf84d-c150-46a7-9129-59c4eee4337f"}}},{"cell_type":"markdown","source":["View the Delta Lake transaction log"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00f874b7-f3ac-444b-b867-e8d33bf99384"}}},{"cell_type":"code","source":["%sql DESCRIBE HISTORY loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b55d9706-57fe-4757-94aa-ea1203c613be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[41,"2021-12-09T15:47:33.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"19"},null,["400448375928372"],"1209-152421-tuxu0dh4",39,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"59500","numOutputBytes":"1320218","numAddedFiles":"1"},null],[40,"2021-12-09T15:47:24.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"20"},null,["400448375928372"],"1209-152421-tuxu0dh4",39,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"20500","numOutputBytes":"470357","numAddedFiles":"1"},null],[39,"2021-12-09T15:46:39.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"18"},null,["400448375928372"],"1209-152421-tuxu0dh4",37,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"58500","numOutputBytes":"1298415","numAddedFiles":"1"},null],[38,"2021-12-09T15:46:02.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"19"},null,["400448375928372"],"1209-152421-tuxu0dh4",37,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"59500","numOutputBytes":"1320176","numAddedFiles":"1"},null],[37,"2021-12-09T15:45:20.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"18"},null,["400448375928372"],"1209-152421-tuxu0dh4",35,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"24000","numOutputBytes":"546702","numAddedFiles":"1"},null],[36,"2021-12-09T15:44:03.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"17"},null,["400448375928372"],"1209-152421-tuxu0dh4",35,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"8500","numOutputBytes":"206293","numAddedFiles":"1"},null],[35,"2021-12-09T15:43:23.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"17"},null,["400448375928372"],"1209-152421-tuxu0dh4",33,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"135037","numAddedFiles":"1"},null],[34,"2021-12-09T15:43:12.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"16"},null,["400448375928372"],"1209-152421-tuxu0dh4",33,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4000","numOutputBytes":"98935","numAddedFiles":"1"},null],[33,"2021-12-09T15:43:06.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"16"},null,["400448375928372"],"1209-152421-tuxu0dh4",31,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"120139","numAddedFiles":"1"},null],[32,"2021-12-09T15:43:01.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"15"},null,["400448375928372"],"1209-152421-tuxu0dh4",31,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"6000","numOutputBytes":"145211","numAddedFiles":"1"},null],[31,"2021-12-09T15:42:55.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"15"},null,["400448375928372"],"1209-152421-tuxu0dh4",29,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"111637","numAddedFiles":"1"},null],[30,"2021-12-09T15:42:51.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"14"},null,["400448375928372"],"1209-152421-tuxu0dh4",29,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"124295","numAddedFiles":"1"},null],[29,"2021-12-09T15:42:45.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"14"},null,["400448375928372"],"1209-152421-tuxu0dh4",27,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122717","numAddedFiles":"1"},null],[28,"2021-12-09T15:42:41.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"13"},null,["400448375928372"],"1209-152421-tuxu0dh4",27,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"111604","numAddedFiles":"1"},null],[27,"2021-12-09T15:42:36.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"13"},null,["400448375928372"],"1209-152421-tuxu0dh4",25,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"109964","numAddedFiles":"1"},null],[26,"2021-12-09T15:42:32.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"12"},null,["400448375928372"],"1209-152421-tuxu0dh4",25,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"109618","numAddedFiles":"1"},null],[25,"2021-12-09T15:42:26.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"12"},null,["400448375928372"],"1209-152421-tuxu0dh4",23,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"108713","numAddedFiles":"1"},null],[24,"2021-12-09T15:42:23.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"11"},null,["400448375928372"],"1209-152421-tuxu0dh4",23,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"3500","numOutputBytes":"86836","numAddedFiles":"1"},null],[23,"2021-12-09T15:42:18.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"11"},null,["400448375928372"],"1209-152421-tuxu0dh4",21,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"135176","numAddedFiles":"1"},null],[22,"2021-12-09T15:42:14.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"10"},null,["400448375928372"],"1209-152421-tuxu0dh4",21,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"6000","numOutputBytes":"144786","numAddedFiles":"1"},null],[21,"2021-12-09T15:42:08.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"10"},null,["400448375928372"],"1209-152421-tuxu0dh4",19,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"123135","numAddedFiles":"1"},null],[20,"2021-12-09T15:42:03.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"9"},null,["400448375928372"],"1209-152421-tuxu0dh4",19,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"134997","numAddedFiles":"1"},null],[19,"2021-12-09T15:41:58.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"9"},null,["400448375928372"],"1209-152421-tuxu0dh4",17,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"123747","numAddedFiles":"1"},null],[18,"2021-12-09T15:41:52.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"8"},null,["400448375928372"],"1209-152421-tuxu0dh4",17,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"121610","numAddedFiles":"1"},null],[17,"2021-12-09T15:41:47.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"8"},null,["400448375928372"],"1209-152421-tuxu0dh4",15,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122744","numAddedFiles":"1"},null],[16,"2021-12-09T15:41:43.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"7"},null,["400448375928372"],"1209-152421-tuxu0dh4",15,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"110042","numAddedFiles":"1"},null],[15,"2021-12-09T15:41:37.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"7"},null,["400448375928372"],"1209-152421-tuxu0dh4",13,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122718","numAddedFiles":"1"},null],[14,"2021-12-09T15:41:33.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"6"},null,["400448375928372"],"1209-152421-tuxu0dh4",13,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"134872","numAddedFiles":"1"},null],[13,"2021-12-09T15:41:28.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"6"},null,["400448375928372"],"1209-152421-tuxu0dh4",11,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"135696","numAddedFiles":"1"},null],[12,"2021-12-09T15:41:22.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"5"},null,["400448375928372"],"1209-152421-tuxu0dh4",11,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"6000","numOutputBytes":"145957","numAddedFiles":"1"},null],[11,"2021-12-09T15:41:16.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"5"},null,["400448375928372"],"1209-152421-tuxu0dh4",10,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"110657","numAddedFiles":"1"},null],[10,"2021-12-09T15:41:11.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"4"},null,["400448375928372"],"1209-152421-tuxu0dh4",8,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"109718","numAddedFiles":"1"},null],[9,"2021-12-09T15:41:06.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"4"},null,["400448375928372"],"1209-152421-tuxu0dh4",8,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"121893","numAddedFiles":"1"},null],[8,"2021-12-09T15:41:01.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"3"},null,["400448375928372"],"1209-152421-tuxu0dh4",6,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122348","numAddedFiles":"1"},null],[7,"2021-12-09T15:40:58.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"3"},null,["400448375928372"],"1209-152421-tuxu0dh4",6,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4000","numOutputBytes":"97207","numAddedFiles":"1"},null],[6,"2021-12-09T15:40:52.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"2"},null,["400448375928372"],"1209-152421-tuxu0dh4",4,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"134456","numAddedFiles":"1"},null],[5,"2021-12-09T15:40:48.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"2"},null,["400448375928372"],"1209-152421-tuxu0dh4",4,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"6500","numOutputBytes":"157004","numAddedFiles":"1"},null],[4,"2021-12-09T15:40:43.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"1"},null,["400448375928372"],"1209-152421-tuxu0dh4",2,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122718","numAddedFiles":"1"},null],[3,"2021-12-09T15:40:39.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"1"},null,["400448375928372"],"1209-152421-tuxu0dh4",2,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"3500","numOutputBytes":"85713","numAddedFiles":"1"},null],[2,"2021-12-09T15:40:30.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"0"},null,["400448375928372"],"1209-152421-tuxu0dh4",0,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"0","numOutputBytes":"789","numAddedFiles":"1"},null],[1,"2021-12-09T15:40:26.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"0"},null,["400448375928372"],"1209-152421-tuxu0dh4",0,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"0","numOutputBytes":"789","numAddedFiles":"1"},null],[0,"2021-12-09T15:38:35.000+0000","492151203200240","kartiksmathur@gmail.com","CREATE OR REPLACE TABLE AS SELECT",{"isManaged":"true","description":null,"partitionBy":"[]","properties":"{\"delta.autoOptimize.optimizeWrite\":\"true\"}"},null,["400448375928372"],"1209-152421-tuxu0dh4",null,"WriteSerializable",false,{"numFiles":"1","numOutputBytes":"165344","numOutputRows":"14705"},null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"userMetadata","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr></thead><tbody><tr><td>41</td><td>2021-12-09T15:47:33.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 19)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>39</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 59500, numOutputBytes -> 1320218, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>40</td><td>2021-12-09T15:47:24.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 20)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>39</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 20500, numOutputBytes -> 470357, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>39</td><td>2021-12-09T15:46:39.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 18)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>37</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 58500, numOutputBytes -> 1298415, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>38</td><td>2021-12-09T15:46:02.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 19)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>37</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 59500, numOutputBytes -> 1320176, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>37</td><td>2021-12-09T15:45:20.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 18)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>35</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 24000, numOutputBytes -> 546702, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>36</td><td>2021-12-09T15:44:03.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 17)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>35</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 8500, numOutputBytes -> 206293, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>35</td><td>2021-12-09T15:43:23.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 17)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>33</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 135037, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>34</td><td>2021-12-09T15:43:12.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 16)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>33</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4000, numOutputBytes -> 98935, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>33</td><td>2021-12-09T15:43:06.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 16)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>31</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 120139, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>32</td><td>2021-12-09T15:43:01.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 15)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>31</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 6000, numOutputBytes -> 145211, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>31</td><td>2021-12-09T15:42:55.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 15)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>29</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 111637, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>30</td><td>2021-12-09T15:42:51.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 14)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>29</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 124295, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>29</td><td>2021-12-09T15:42:45.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 14)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>27</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122717, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>28</td><td>2021-12-09T15:42:41.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 13)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>27</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 111604, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>27</td><td>2021-12-09T15:42:36.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 13)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>25</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 109964, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>26</td><td>2021-12-09T15:42:32.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 12)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>25</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 109618, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>25</td><td>2021-12-09T15:42:26.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 12)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>23</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 108713, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>24</td><td>2021-12-09T15:42:23.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 11)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>23</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 3500, numOutputBytes -> 86836, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>23</td><td>2021-12-09T15:42:18.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 11)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>21</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 135176, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>22</td><td>2021-12-09T15:42:14.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 10)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>21</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 6000, numOutputBytes -> 144786, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>21</td><td>2021-12-09T15:42:08.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 10)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>19</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 123135, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>20</td><td>2021-12-09T15:42:03.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 9)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>19</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 134997, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>19</td><td>2021-12-09T15:41:58.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 9)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>17</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 123747, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>18</td><td>2021-12-09T15:41:52.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 8)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>17</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 121610, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>17</td><td>2021-12-09T15:41:47.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 8)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>15</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122744, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>16</td><td>2021-12-09T15:41:43.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 7)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>15</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 110042, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>15</td><td>2021-12-09T15:41:37.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 7)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>13</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122718, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>14</td><td>2021-12-09T15:41:33.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 6)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>13</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 134872, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>13</td><td>2021-12-09T15:41:28.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 6)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>11</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 135696, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>12</td><td>2021-12-09T15:41:22.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 5)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>11</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 6000, numOutputBytes -> 145957, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>11</td><td>2021-12-09T15:41:16.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 5)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>10</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 110657, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>10</td><td>2021-12-09T15:41:11.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 4)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>8</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 109718, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>9</td><td>2021-12-09T15:41:06.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 4)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>8</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 121893, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>8</td><td>2021-12-09T15:41:01.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 3)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>6</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122348, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>7</td><td>2021-12-09T15:40:58.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 3)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>6</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4000, numOutputBytes -> 97207, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>6</td><td>2021-12-09T15:40:52.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 2)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>4</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 134456, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>5</td><td>2021-12-09T15:40:48.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 2)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>4</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 6500, numOutputBytes -> 157004, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>4</td><td>2021-12-09T15:40:43.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 1)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>2</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122718, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>3</td><td>2021-12-09T15:40:39.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 1)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>2</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 3500, numOutputBytes -> 85713, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>2</td><td>2021-12-09T15:40:30.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 0)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 0, numOutputBytes -> 789, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>1</td><td>2021-12-09T15:40:26.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 0)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 0, numOutputBytes -> 789, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>0</td><td>2021-12-09T15:38:35.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {\"delta.autoOptimize.optimizeWrite\":\"true\"})</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputBytes -> 165344, numOutputRows -> 14705)</td><td>null</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://databricks.com/wp-content/uploads/2020/09/delta-lake-medallion-model-scaled.jpg\" width=1012/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16b9d445-9ad0-4618-b4f4-01659a789af6"}}},{"cell_type":"markdown","source":["##  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Use Schema Enforcement to protect data quality"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d5b46f8-2e68-4412-ac18-cd1a9921668d"}}},{"cell_type":"markdown","source":["To show you how schema enforcement works, let's create a new table that has an extra column -- `credit_score` -- that doesn't match our existing Delta Lake table schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32f05777-dcf3-4926-9036-7c9b4232eada"}}},{"cell_type":"markdown","source":["#### Write DataFrame with extra column, `credit_score`, to Delta Lake table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9006c73a-8716-4f9b-a3fa-ba7ca1bafadb"}}},{"cell_type":"code","source":["# Generate `new_data` with additional column\nnew_column = [StructField(\"credit_score\", IntegerType(), True)]\nnew_schema = StructType(spark.table(\"loans_delta\").schema.fields + new_column)\ndata = [(99997, 10000, 1338.55, \"CA\", \"batch\", datetime.now(), 649),\n        (99998, 20000, 1442.55, \"NY\", \"batch\", datetime.now(), 702)]\n\nnew_data = spark.createDataFrame(data, new_schema)\nnew_data.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16c2db2a-d2ae-4b22-a10d-c51405f3f584"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"new_data","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"loan_id","nullable":true,"type":"long"},{"metadata":{},"name":"funded_amnt","nullable":true,"type":"integer"},{"metadata":{},"name":"paid_amnt","nullable":true,"type":"double"},{"metadata":{},"name":"addr_state","nullable":true,"type":"string"},{"metadata":{},"name":"type","nullable":true,"type":"string"},{"metadata":{},"name":"timestamp","nullable":true,"type":"timestamp"},{"metadata":{},"name":"credit_score","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- loan_id: long (nullable = true)\n |-- funded_amnt: integer (nullable = true)\n |-- paid_amnt: double (nullable = true)\n |-- addr_state: string (nullable = true)\n |-- type: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- credit_score: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- type: string (nullable = true)\n-- timestamp: timestamp (nullable = true)\n-- credit_score: integer (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Uncommenting this cell will lead to an error because the schemas don't match.\n# Attempt to write data with new column to Delta Lake table\nnew_data.write.format(\"delta\").mode(\"append\").saveAsTable(\"loans_delta\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b01b9a7d-f3dc-47e0-8388-88cffee41c84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-400448375928409&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Uncommenting this cell will lead to an error because the schemas don&#39;t match.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Attempt to write data with new column to Delta Lake table</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>new_data<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;delta&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;append&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>saveAsTable<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;loans_delta&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">saveAsTable</span><span class=\"ansi-blue-fg\">(self, name, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    866</span>         <span class=\"ansi-green-fg\">if</span> format <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    867</span>             self<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>format<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 868</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>saveAsTable<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    869</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    870</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 133</span><span class=\"ansi-red-fg\">                 </span>raise_from<span class=\"ansi-blue-fg\">(</span>converted<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    134</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">raise_from</span><span class=\"ansi-blue-fg\">(e)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: A schema mismatch detected when writing to the Delta table (Table ID: 652f730b-5d0d-49eb-b58f-9bcf5f7a123c).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n&#39;.option(&#34;mergeSchema&#34;, &#34;true&#34;)&#39;.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to &#34;true&#34;. See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- type: string (nullable = true)\n-- timestamp: timestamp (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- type: string (nullable = true)\n-- timestamp: timestamp (nullable = true)\n-- credit_score: integer (nullable = true)\n\n         ;</div>","errorSummary":"<span class=\"ansi-red-fg\">AnalysisException</span>: A schema mismatch detected when writing to the Delta table (Table ID: 652f730b-5d0d-49eb-b58f-9bcf5f7a123c).","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-400448375928409&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Uncommenting this cell will lead to an error because the schemas don&#39;t match.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># Attempt to write data with new column to Delta Lake table</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>new_data<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;delta&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;append&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>saveAsTable<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;loans_delta&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">saveAsTable</span><span class=\"ansi-blue-fg\">(self, name, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    866</span>         <span class=\"ansi-green-fg\">if</span> format <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    867</span>             self<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>format<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 868</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>saveAsTable<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    869</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    870</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 133</span><span class=\"ansi-red-fg\">                 </span>raise_from<span class=\"ansi-blue-fg\">(</span>converted<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    134</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">raise_from</span><span class=\"ansi-blue-fg\">(e)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: A schema mismatch detected when writing to the Delta table (Table ID: 652f730b-5d0d-49eb-b58f-9bcf5f7a123c).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n&#39;.option(&#34;mergeSchema&#34;, &#34;true&#34;)&#39;.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to &#34;true&#34;. See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- type: string (nullable = true)\n-- timestamp: timestamp (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- type: string (nullable = true)\n-- timestamp: timestamp (nullable = true)\n-- credit_score: integer (nullable = true)\n\n         ;</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["**Schema enforcement helps keep our tables clean and tidy so that we can trust the data we have stored in Delta Lake.** The writes above were blocked because the schema of the new data did not match the schema of table (see the exception details). See more information about how it works [here](https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0510e679-0bda-438e-bafc-bb245b306aeb"}}},{"cell_type":"markdown","source":["##  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Use Schema Evolution to add new columns to schema\n\nIf we *want* to update our Delta Lake table to match this data source's schema, we can do so using schema evolution. Simply add the following to the Spark write command: `.option(\"mergeSchema\", \"true\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4919cc33-799c-4021-9fba-cf257fb1f451"}}},{"cell_type":"code","source":["new_data.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"loans_delta\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fe4a3d3-b158-4a7c-9283-29bfd574f58d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%sql SELECT * FROM loans_delta WHERE loan_id IN (99997, 99998)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4537592-fdc0-4e8b-a99b-5cdc009d5c37"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[99997,6301,5875.6904049278555,"CA","stream B","2021-12-09T15:43:24.324+0000",null],[99998,7893,6073.122435489927,"TX","stream B","2021-12-09T15:43:24.326+0000",null],[99998,5648,4025.023474160944,"TX","stream A","2021-12-09T15:43:22.723+0000",null],[99997,5580,4191.152712701205,"CA","stream A","2021-12-09T15:43:22.721+0000",null],[99997,10000,1338.55,"CA","batch","2021-12-09T15:48:10.227+0000",649],[99998,20000,1442.55,"NY","batch","2021-12-09T15:48:10.227+0000",702]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"loan_id","type":"\"long\"","metadata":"{}"},{"name":"funded_amnt","type":"\"integer\"","metadata":"{}"},{"name":"paid_amnt","type":"\"double\"","metadata":"{}"},{"name":"addr_state","type":"\"string\"","metadata":"{}"},{"name":"type","type":"\"string\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"credit_score","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>loan_id</th><th>funded_amnt</th><th>paid_amnt</th><th>addr_state</th><th>type</th><th>timestamp</th><th>credit_score</th></tr></thead><tbody><tr><td>99997</td><td>6301</td><td>5875.6904049278555</td><td>CA</td><td>stream B</td><td>2021-12-09T15:43:24.324+0000</td><td>null</td></tr><tr><td>99998</td><td>7893</td><td>6073.122435489927</td><td>TX</td><td>stream B</td><td>2021-12-09T15:43:24.326+0000</td><td>null</td></tr><tr><td>99998</td><td>5648</td><td>4025.023474160944</td><td>TX</td><td>stream A</td><td>2021-12-09T15:43:22.723+0000</td><td>null</td></tr><tr><td>99997</td><td>5580</td><td>4191.152712701205</td><td>CA</td><td>stream A</td><td>2021-12-09T15:43:22.721+0000</td><td>null</td></tr><tr><td>99997</td><td>10000</td><td>1338.55</td><td>CA</td><td>batch</td><td>2021-12-09T15:48:10.227+0000</td><td>649</td></tr><tr><td>99998</td><td>20000</td><td>1442.55</td><td>NY</td><td>batch</td><td>2021-12-09T15:48:10.227+0000</td><td>702</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Delta Lake Time Travel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47a1482d-3bd8-4f13-af15-aa7a5351fa2e"}}},{"cell_type":"markdown","source":["Delta Lake’s time travel capabilities simplify building data pipelines for use cases including:\n\n* Auditing Data Changes\n* Reproducing experiments & reports\n* Rollbacks\n\nAs you write into a Delta table or directory, every operation is automatically versioned.\n\n<img src=\"https://github.com/risan4841/img/blob/master/transactionallogs.png?raw=true\" width=250/>\n\nYou can query snapshots of your tables by:\n1. **Version number**, or\n2. **Timestamp.**\n\nusing Python, Scala, and/or SQL syntax; for these examples we will use the SQL syntax.  \n\nFor more information, refer to the [docs](https://docs.delta.io/latest/delta-utility.html#history), or [Introducing Delta Time Travel for Large Scale Data Lakes](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6f79555-0501-41a3-9296-4943ee2d6d8e"}}},{"cell_type":"markdown","source":["#### Review Delta Lake Table History for  Auditing & Governance\nAll the transactions for this table are stored within this table including the initial set of insertions, update, delete, merge, and inserts with schema modification"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18ef5f40-ecaa-4fc2-b57b-e3bd97f0f409"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"687a343a-43ff-4a8f-90ea-cd0ae411ca44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[42,"2021-12-09T15:48:14.000+0000","492151203200240","kartiksmathur@gmail.com","WRITE",{"mode":"Append","partitionBy":"[]"},null,["400448375928372"],"1209-152421-tuxu0dh4",41,"WriteSerializable",true,{"numFiles":"1","numOutputBytes":"1969","numOutputRows":"2"},null],[41,"2021-12-09T15:47:33.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"19"},null,["400448375928372"],"1209-152421-tuxu0dh4",39,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"59500","numOutputBytes":"1320218","numAddedFiles":"1"},null],[40,"2021-12-09T15:47:24.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"20"},null,["400448375928372"],"1209-152421-tuxu0dh4",39,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"20500","numOutputBytes":"470357","numAddedFiles":"1"},null],[39,"2021-12-09T15:46:39.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"18"},null,["400448375928372"],"1209-152421-tuxu0dh4",37,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"58500","numOutputBytes":"1298415","numAddedFiles":"1"},null],[38,"2021-12-09T15:46:02.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"19"},null,["400448375928372"],"1209-152421-tuxu0dh4",37,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"59500","numOutputBytes":"1320176","numAddedFiles":"1"},null],[37,"2021-12-09T15:45:20.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"18"},null,["400448375928372"],"1209-152421-tuxu0dh4",35,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"24000","numOutputBytes":"546702","numAddedFiles":"1"},null],[36,"2021-12-09T15:44:03.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"17"},null,["400448375928372"],"1209-152421-tuxu0dh4",35,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"8500","numOutputBytes":"206293","numAddedFiles":"1"},null],[35,"2021-12-09T15:43:23.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"17"},null,["400448375928372"],"1209-152421-tuxu0dh4",33,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"135037","numAddedFiles":"1"},null],[34,"2021-12-09T15:43:12.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"16"},null,["400448375928372"],"1209-152421-tuxu0dh4",33,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4000","numOutputBytes":"98935","numAddedFiles":"1"},null],[33,"2021-12-09T15:43:06.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"16"},null,["400448375928372"],"1209-152421-tuxu0dh4",31,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"120139","numAddedFiles":"1"},null],[32,"2021-12-09T15:43:01.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"15"},null,["400448375928372"],"1209-152421-tuxu0dh4",31,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"6000","numOutputBytes":"145211","numAddedFiles":"1"},null],[31,"2021-12-09T15:42:55.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"15"},null,["400448375928372"],"1209-152421-tuxu0dh4",29,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"111637","numAddedFiles":"1"},null],[30,"2021-12-09T15:42:51.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"14"},null,["400448375928372"],"1209-152421-tuxu0dh4",29,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"124295","numAddedFiles":"1"},null],[29,"2021-12-09T15:42:45.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"14"},null,["400448375928372"],"1209-152421-tuxu0dh4",27,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122717","numAddedFiles":"1"},null],[28,"2021-12-09T15:42:41.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"13"},null,["400448375928372"],"1209-152421-tuxu0dh4",27,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"111604","numAddedFiles":"1"},null],[27,"2021-12-09T15:42:36.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"13"},null,["400448375928372"],"1209-152421-tuxu0dh4",25,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"109964","numAddedFiles":"1"},null],[26,"2021-12-09T15:42:32.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"12"},null,["400448375928372"],"1209-152421-tuxu0dh4",25,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"109618","numAddedFiles":"1"},null],[25,"2021-12-09T15:42:26.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"12"},null,["400448375928372"],"1209-152421-tuxu0dh4",23,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"108713","numAddedFiles":"1"},null],[24,"2021-12-09T15:42:23.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"11"},null,["400448375928372"],"1209-152421-tuxu0dh4",23,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"3500","numOutputBytes":"86836","numAddedFiles":"1"},null],[23,"2021-12-09T15:42:18.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"11"},null,["400448375928372"],"1209-152421-tuxu0dh4",21,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"135176","numAddedFiles":"1"},null],[22,"2021-12-09T15:42:14.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"10"},null,["400448375928372"],"1209-152421-tuxu0dh4",21,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"6000","numOutputBytes":"144786","numAddedFiles":"1"},null],[21,"2021-12-09T15:42:08.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"10"},null,["400448375928372"],"1209-152421-tuxu0dh4",19,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"123135","numAddedFiles":"1"},null],[20,"2021-12-09T15:42:03.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"9"},null,["400448375928372"],"1209-152421-tuxu0dh4",19,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"134997","numAddedFiles":"1"},null],[19,"2021-12-09T15:41:58.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"9"},null,["400448375928372"],"1209-152421-tuxu0dh4",17,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"123747","numAddedFiles":"1"},null],[18,"2021-12-09T15:41:52.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"8"},null,["400448375928372"],"1209-152421-tuxu0dh4",17,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"121610","numAddedFiles":"1"},null],[17,"2021-12-09T15:41:47.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"8"},null,["400448375928372"],"1209-152421-tuxu0dh4",15,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122744","numAddedFiles":"1"},null],[16,"2021-12-09T15:41:43.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"7"},null,["400448375928372"],"1209-152421-tuxu0dh4",15,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"110042","numAddedFiles":"1"},null],[15,"2021-12-09T15:41:37.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"7"},null,["400448375928372"],"1209-152421-tuxu0dh4",13,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122718","numAddedFiles":"1"},null],[14,"2021-12-09T15:41:33.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"6"},null,["400448375928372"],"1209-152421-tuxu0dh4",13,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"134872","numAddedFiles":"1"},null],[13,"2021-12-09T15:41:28.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"6"},null,["400448375928372"],"1209-152421-tuxu0dh4",11,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"135696","numAddedFiles":"1"},null],[12,"2021-12-09T15:41:22.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"5"},null,["400448375928372"],"1209-152421-tuxu0dh4",11,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"6000","numOutputBytes":"145957","numAddedFiles":"1"},null],[11,"2021-12-09T15:41:16.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"5"},null,["400448375928372"],"1209-152421-tuxu0dh4",10,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"110657","numAddedFiles":"1"},null],[10,"2021-12-09T15:41:11.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"4"},null,["400448375928372"],"1209-152421-tuxu0dh4",8,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4500","numOutputBytes":"109718","numAddedFiles":"1"},null],[9,"2021-12-09T15:41:06.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"4"},null,["400448375928372"],"1209-152421-tuxu0dh4",8,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"121893","numAddedFiles":"1"},null],[8,"2021-12-09T15:41:01.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"3"},null,["400448375928372"],"1209-152421-tuxu0dh4",6,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122348","numAddedFiles":"1"},null],[7,"2021-12-09T15:40:58.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"3"},null,["400448375928372"],"1209-152421-tuxu0dh4",6,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"4000","numOutputBytes":"97207","numAddedFiles":"1"},null],[6,"2021-12-09T15:40:52.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"2"},null,["400448375928372"],"1209-152421-tuxu0dh4",4,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5500","numOutputBytes":"134456","numAddedFiles":"1"},null],[5,"2021-12-09T15:40:48.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"2"},null,["400448375928372"],"1209-152421-tuxu0dh4",4,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"6500","numOutputBytes":"157004","numAddedFiles":"1"},null],[4,"2021-12-09T15:40:43.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"1"},null,["400448375928372"],"1209-152421-tuxu0dh4",2,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"5000","numOutputBytes":"122718","numAddedFiles":"1"},null],[3,"2021-12-09T15:40:39.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"1"},null,["400448375928372"],"1209-152421-tuxu0dh4",2,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"3500","numOutputBytes":"85713","numAddedFiles":"1"},null],[2,"2021-12-09T15:40:30.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"10fb1838-3701-4e5a-bb06-aa90c20b0f7e","epochId":"0"},null,["400448375928372"],"1209-152421-tuxu0dh4",0,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"0","numOutputBytes":"789","numAddedFiles":"1"},null],[1,"2021-12-09T15:40:26.000+0000","492151203200240","kartiksmathur@gmail.com","STREAMING UPDATE",{"outputMode":"Append","queryId":"bcc606b9-fc3c-4816-a6ed-ba62154b3be4","epochId":"0"},null,["400448375928372"],"1209-152421-tuxu0dh4",0,"WriteSerializable",true,{"numRemovedFiles":"0","numOutputRows":"0","numOutputBytes":"789","numAddedFiles":"1"},null],[0,"2021-12-09T15:38:35.000+0000","492151203200240","kartiksmathur@gmail.com","CREATE OR REPLACE TABLE AS SELECT",{"isManaged":"true","description":null,"partitionBy":"[]","properties":"{\"delta.autoOptimize.optimizeWrite\":\"true\"}"},null,["400448375928372"],"1209-152421-tuxu0dh4",null,"WriteSerializable",false,{"numFiles":"1","numOutputBytes":"165344","numOutputRows":"14705"},null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"userMetadata","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th></tr></thead><tbody><tr><td>42</td><td>2021-12-09T15:48:14.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>41</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputBytes -> 1969, numOutputRows -> 2)</td><td>null</td></tr><tr><td>41</td><td>2021-12-09T15:47:33.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 19)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>39</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 59500, numOutputBytes -> 1320218, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>40</td><td>2021-12-09T15:47:24.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 20)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>39</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 20500, numOutputBytes -> 470357, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>39</td><td>2021-12-09T15:46:39.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 18)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>37</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 58500, numOutputBytes -> 1298415, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>38</td><td>2021-12-09T15:46:02.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 19)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>37</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 59500, numOutputBytes -> 1320176, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>37</td><td>2021-12-09T15:45:20.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 18)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>35</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 24000, numOutputBytes -> 546702, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>36</td><td>2021-12-09T15:44:03.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 17)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>35</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 8500, numOutputBytes -> 206293, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>35</td><td>2021-12-09T15:43:23.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 17)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>33</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 135037, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>34</td><td>2021-12-09T15:43:12.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 16)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>33</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4000, numOutputBytes -> 98935, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>33</td><td>2021-12-09T15:43:06.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 16)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>31</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 120139, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>32</td><td>2021-12-09T15:43:01.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 15)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>31</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 6000, numOutputBytes -> 145211, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>31</td><td>2021-12-09T15:42:55.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 15)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>29</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 111637, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>30</td><td>2021-12-09T15:42:51.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 14)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>29</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 124295, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>29</td><td>2021-12-09T15:42:45.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 14)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>27</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122717, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>28</td><td>2021-12-09T15:42:41.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 13)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>27</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 111604, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>27</td><td>2021-12-09T15:42:36.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 13)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>25</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 109964, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>26</td><td>2021-12-09T15:42:32.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 12)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>25</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 109618, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>25</td><td>2021-12-09T15:42:26.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 12)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>23</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 108713, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>24</td><td>2021-12-09T15:42:23.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 11)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>23</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 3500, numOutputBytes -> 86836, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>23</td><td>2021-12-09T15:42:18.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 11)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>21</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 135176, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>22</td><td>2021-12-09T15:42:14.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 10)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>21</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 6000, numOutputBytes -> 144786, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>21</td><td>2021-12-09T15:42:08.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 10)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>19</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 123135, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>20</td><td>2021-12-09T15:42:03.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 9)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>19</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 134997, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>19</td><td>2021-12-09T15:41:58.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 9)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>17</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 123747, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>18</td><td>2021-12-09T15:41:52.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 8)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>17</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 121610, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>17</td><td>2021-12-09T15:41:47.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 8)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>15</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122744, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>16</td><td>2021-12-09T15:41:43.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 7)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>15</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 110042, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>15</td><td>2021-12-09T15:41:37.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 7)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>13</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122718, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>14</td><td>2021-12-09T15:41:33.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 6)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>13</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 134872, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>13</td><td>2021-12-09T15:41:28.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 6)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>11</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 135696, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>12</td><td>2021-12-09T15:41:22.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 5)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>11</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 6000, numOutputBytes -> 145957, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>11</td><td>2021-12-09T15:41:16.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 5)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>10</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 110657, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>10</td><td>2021-12-09T15:41:11.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 4)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>8</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4500, numOutputBytes -> 109718, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>9</td><td>2021-12-09T15:41:06.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 4)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>8</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 121893, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>8</td><td>2021-12-09T15:41:01.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 3)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>6</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122348, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>7</td><td>2021-12-09T15:40:58.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 3)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>6</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 4000, numOutputBytes -> 97207, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>6</td><td>2021-12-09T15:40:52.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 2)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>4</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5500, numOutputBytes -> 134456, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>5</td><td>2021-12-09T15:40:48.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 2)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>4</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 6500, numOutputBytes -> 157004, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>4</td><td>2021-12-09T15:40:43.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 1)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>2</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 5000, numOutputBytes -> 122718, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>3</td><td>2021-12-09T15:40:39.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 1)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>2</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 3500, numOutputBytes -> 85713, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>2</td><td>2021-12-09T15:40:30.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> 10fb1838-3701-4e5a-bb06-aa90c20b0f7e, epochId -> 0)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 0, numOutputBytes -> 789, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>1</td><td>2021-12-09T15:40:26.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>STREAMING UPDATE</td><td>Map(outputMode -> Append, queryId -> bcc606b9-fc3c-4816-a6ed-ba62154b3be4, epochId -> 0)</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numRemovedFiles -> 0, numOutputRows -> 0, numOutputBytes -> 789, numAddedFiles -> 1)</td><td>null</td></tr><tr><td>0</td><td>2021-12-09T15:38:35.000+0000</td><td>492151203200240</td><td>kartiksmathur@gmail.com</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {\"delta.autoOptimize.optimizeWrite\":\"true\"})</td><td>null</td><td>List(400448375928372)</td><td>1209-152421-tuxu0dh4</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputBytes -> 165344, numOutputRows -> 14705)</td><td>null</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### Use time travel to select and view the original version of our table (Version 0).\nAs you can see, this version contains the original 14,705 records in it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6081d55-b759-44d2-bbb6-21cbe18ff79e"}}},{"cell_type":"code","source":["spark.sql(\"SELECT * FROM loans_delta VERSION AS OF 0\").show(3)\nspark.sql(\"SELECT COUNT(*) FROM loans_delta VERSION AS OF 0\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"540e9846-dd77-4965-997c-9afd59e270c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-----------+---------+----------+-----+--------------------+\n|loan_id|funded_amnt|paid_amnt|addr_state| type|           timestamp|\n+-------+-----------+---------+----------+-----+--------------------+\n|      0|       1000|   182.22|        CA|batch|2021-12-09 15:38:...|\n|      1|       1000|   361.19|        WA|batch|2021-12-09 15:38:...|\n|      2|       1000|   176.26|        TX|batch|2021-12-09 15:38:...|\n+-------+-----------+---------+----------+-----+--------------------+\nonly showing top 3 rows\n\n+--------+\n|count(1)|\n+--------+\n|   14705|\n+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------+---------+----------+-----+--------------------+\nloan_id|funded_amnt|paid_amnt|addr_state| type|           timestamp|\n+-------+-----------+---------+----------+-----+--------------------+\n      0|       1000|   182.22|        CA|batch|2021-12-09 15:38:...|\n      1|       1000|   361.19|        WA|batch|2021-12-09 15:38:...|\n      2|       1000|   176.26|        TX|batch|2021-12-09 15:38:...|\n+-------+-----------+---------+----------+-----+--------------------+\nonly showing top 3 rows\n\n+--------+\ncount(1)|\n+--------+\n   14705|\n+--------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%sql SELECT COUNT(*) FROM loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6163a99-e301-48ff-94c8-f802319fc2c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[408707]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"count(1)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>408707</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### Rollback a table to a specific version using `RESTORE`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edf8ad66-38f9-46fc-84c5-0af7f3792863"}}},{"cell_type":"code","source":["%sql RESTORE loans_delta VERSION AS OF 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03b3bd3a-6cf9-442d-ad10-a1cefe0e1c6f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input 'RESTORE' expecting {'(', 'CONVERT', 'COPY', 'OPTIMIZE', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\n\n== SQL ==\nRESTORE loans_delta VERSION AS OF 0\n^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:133)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:55)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:81)\n\tat com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:70)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:90)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:67)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:678)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:678)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: ParseException: \nmismatched input 'RESTORE' expecting {'(', 'CONVERT', 'COPY', 'OPTIMIZE', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\n\n== SQL ==\nRESTORE loans_delta VERSION AS OF 0\n^^^\n","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input 'RESTORE' expecting {'(', 'CONVERT', 'COPY', 'OPTIMIZE', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\n\n== SQL ==\nRESTORE loans_delta VERSION AS OF 0\n^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:133)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:55)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:81)\n\tat com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:70)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:90)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:67)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:678)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:678)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%sql SELECT COUNT(*) FROM loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbb59908-891a-4ab8-90e3-2979f23b4810"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[408707]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"count(1)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>408707</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["##![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Full DML Support: `DELETE`, `UPDATE`, `MERGE INTO`\n\nDelta Lake brings ACID transactions and full DML support to data lakes.\n\n>Parquet does **not** support these commands - they are unique to Delta Lake."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"506f396b-499a-4b85-a9e7-a51869b42ac1"}}},{"cell_type":"markdown","source":["###![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) `DELETE`: Handle GDPR or CCPA Requests on your Data Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1ed25a6-6402-45ca-b19d-2480b529ef5f"}}},{"cell_type":"markdown","source":["Imagine that we are responding to a GDPR data deletion request. The user with loan ID #4420 wants us to delete their data. Here's how easy it is."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"faf9ad37-3db5-44ee-b604-4ca330a0bec5"}}},{"cell_type":"markdown","source":["**View the user's data**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27a902fd-dccb-4d76-b2c0-68422bb811c9"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM loans_delta WHERE loan_id=4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1e1f6d9-0717-4180-8003-e24579ca4287"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[4420,22000,1050.94,"TX","batch","2021-12-09T15:38:32.015+0000",null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"loan_id","type":"\"long\"","metadata":"{}"},{"name":"funded_amnt","type":"\"integer\"","metadata":"{}"},{"name":"paid_amnt","type":"\"double\"","metadata":"{}"},{"name":"addr_state","type":"\"string\"","metadata":"{}"},{"name":"type","type":"\"string\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"credit_score","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>loan_id</th><th>funded_amnt</th><th>paid_amnt</th><th>addr_state</th><th>type</th><th>timestamp</th><th>credit_score</th></tr></thead><tbody><tr><td>4420</td><td>22000</td><td>1050.94</td><td>TX</td><td>batch</td><td>2021-12-09T15:38:32.015+0000</td><td>null</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["**Delete the individual user's data with a single `DELETE` command using Delta Lake.**\n\nNote: The `DELETE` command isn't supported in Parquet."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c15243d5-24be-45c7-be26-cd0b4a1af666"}}},{"cell_type":"code","source":["%sql\nDELETE FROM loans_delta WHERE loan_id=4420;\n-- Confirm the user's data was deleted\nSELECT * FROM loans_delta WHERE loan_id=4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f231704-8ea6-4467-b762-58e437850c68"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"loan_id","type":"\"long\"","metadata":"{}"},{"name":"funded_amnt","type":"\"integer\"","metadata":"{}"},{"name":"paid_amnt","type":"\"double\"","metadata":"{}"},{"name":"addr_state","type":"\"string\"","metadata":"{}"},{"name":"type","type":"\"string\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"credit_score","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>loan_id</th><th>funded_amnt</th><th>paid_amnt</th><th>addr_state</th><th>type</th><th>timestamp</th><th>credit_score</th></tr></thead><tbody></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["###![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Use time travel and `INSERT INTO` to add the user back into our table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4dddce4d-11d2-4df8-9814-8ac06a9c7264"}}},{"cell_type":"code","source":["%sql\nINSERT INTO loans_delta\nSELECT * FROM loans_delta VERSION AS OF 0\nWHERE loan_id=4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da287d9-e739-46be-9f67-fb0ce9af8a03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Cannot write to 'deltadb.loans_delta', not enough data columns; target table has 7 column(s) but the inserted data has 6 column(s);\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.notEnoughColumnsInInsert(DeltaErrors.scala:372)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.com$databricks$sql$transaction$tahoe$DeltaAnalysis$$needsSchemaAdjustment(DeltaAnalysis.scala:397)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:60)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:57)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:216)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:57)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:52)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:152)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:149)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:141)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:217)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:180)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:119)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:119)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:200)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:91)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:171)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:81)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: AnalysisException: Cannot write to 'deltadb.loans_delta', not enough data columns; target table has 7 column(s) but the inserted data has 6 column(s);","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Cannot write to 'deltadb.loans_delta', not enough data columns; target table has 7 column(s) but the inserted data has 6 column(s);\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.notEnoughColumnsInInsert(DeltaErrors.scala:372)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.com$databricks$sql$transaction$tahoe$DeltaAnalysis$$needsSchemaAdjustment(DeltaAnalysis.scala:397)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:60)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:57)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:216)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:57)\n\tat com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:52)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:152)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:149)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:141)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:217)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:180)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:119)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:119)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:200)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:91)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:171)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:81)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%sql SELECT * FROM loans_delta WHERE loan_id=4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50f46f4c-f3f7-4c78-9f9f-74a36d848fcf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"loan_id","type":"\"long\"","metadata":"{}"},{"name":"funded_amnt","type":"\"integer\"","metadata":"{}"},{"name":"paid_amnt","type":"\"double\"","metadata":"{}"},{"name":"addr_state","type":"\"string\"","metadata":"{}"},{"name":"type","type":"\"string\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"credit_score","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>loan_id</th><th>funded_amnt</th><th>paid_amnt</th><th>addr_state</th><th>type</th><th>timestamp</th><th>credit_score</th></tr></thead><tbody></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### ![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) `UPDATE`: Modify the existing records in a table in one command"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"513ffd4e-13be-4f24-bc9d-85b13fe7b7ba"}}},{"cell_type":"code","source":["%sql UPDATE loans_delta SET funded_amnt = 22000 WHERE loan_id = 4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce7d6d35-853c-4555-8e1c-efd3eddd475e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%sql SELECT * FROM loans_delta WHERE loan_id = 4420"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef8d6b29-9c07-46de-988c-014fc5abde42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"loan_id","type":"\"long\"","metadata":"{}"},{"name":"funded_amnt","type":"\"integer\"","metadata":"{}"},{"name":"paid_amnt","type":"\"double\"","metadata":"{}"},{"name":"addr_state","type":"\"string\"","metadata":"{}"},{"name":"type","type":"\"string\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"credit_score","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>loan_id</th><th>funded_amnt</th><th>paid_amnt</th><th>addr_state</th><th>type</th><th>timestamp</th><th>credit_score</th></tr></thead><tbody></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["###![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Support Change Data Capture Workflows & Other Ingest Use Cases via `MERGE INTO`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f340c99-442a-49f4-9e6d-69d8336205d5"}}},{"cell_type":"markdown","source":["With a legacy data pipeline, to insert or update a table, you must:\n1. Identify the new rows to be inserted\n2. Identify the rows that will be replaced (i.e. updated)\n3. Identify all of the rows that are not impacted by the insert or update\n4. Create a new temp based on all three insert statements\n5. Delete the original table (and all of those associated files)\n6. \"Rename\" the temp table back to the original table name\n7. Drop the temp table\n\n<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/merge-into-legacy.gif\" alt='Merge process' width=600/>\n\n\n#### INSERT or UPDATE with Delta Lake\n\n2-step process: \n1. Identify rows to insert or update\n2. Use `MERGE`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d53ddd5d-3b88-4b5c-b5ee-9d5e0709e737"}}},{"cell_type":"code","source":["# Create merge table with 1 row update, 1 insertion\ndata = [(4420, 22000, 21500.00, \"NY\", \"update\", datetime.now()),  # record to update\n        (99999, 10000, 1338.55, \"CA\", \"insert\", datetime.now())]  # record to insert\nschema = spark.table(\"loans_delta\").schema\nspark.createDataFrame(data, schema).createOrReplaceTempView(\"merge_table\")\nspark.sql(\"SELECT * FROM merge_table\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e58702f-a117-4202-9ae3-ae4faff11db8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-400448375928439&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>         (99999, 10000, 1338.55, &#34;CA&#34;, &#34;insert&#34;, datetime.now())]  # record to insert\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> schema <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>table<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;loans_delta&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>schema\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>createOrReplaceTempView<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;merge_table&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;SELECT * FROM merge_table&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/databricks/utils/instrumentation.py</span> in <span class=\"ansi-cyan-fg\">wrapper</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     40</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span>             start_time <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 42</span><span class=\"ansi-red-fg\">             </span>return_val <span class=\"ansi-blue-fg\">=</span> func<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     43</span>         <span class=\"ansi-green-fg\">except</span> Exception<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span>             duration <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span>time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-</span> start_time<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">1000</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    656</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-intense-fg ansi-bold\">    657</span>                 data, schema, samplingRatio, verifySchema)\n<span class=\"ansi-green-fg\">--&gt; 658</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    659</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    660</span>     <span class=\"ansi-green-fg\">def</span> _create_dataframe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    688</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    689</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 690</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    691</span>             jrdd <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>SerDeUtil<span class=\"ansi-blue-fg\">.</span>toJavaArray<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">.</span>_to_java_object_rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    692</span>             jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>applySchemaToPythonRDD<span class=\"ansi-blue-fg\">(</span>jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">.</span>json<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromLocal</span><span class=\"ansi-blue-fg\">(self, data, schema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    472</span>         write temp files<span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    473</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 474</span><span class=\"ansi-red-fg\">         </span>data<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_wrap_data_schema<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    475</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>parallelize<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema\n<span class=\"ansi-green-intense-fg ansi-bold\">    476</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_wrap_data_schema</span><span class=\"ansi-blue-fg\">(self, data, schema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    448</span>         <span class=\"ansi-red-fg\"># make sure data could consumed multiple times</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    449</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-green-fg\">not</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> list<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 450</span><span class=\"ansi-red-fg\">             </span>data <span class=\"ansi-blue-fg\">=</span> list<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    451</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    452</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">prepare</span><span class=\"ansi-blue-fg\">(obj)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    663</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    664</span>             <span class=\"ansi-green-fg\">def</span> prepare<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 665</span><span class=\"ansi-red-fg\">                 </span>verify_func<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    666</span>                 <span class=\"ansi-green-fg\">return</span> obj\n<span class=\"ansi-green-intense-fg ansi-bold\">    667</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">verify</span><span class=\"ansi-blue-fg\">(obj)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1406</span>     <span class=\"ansi-green-fg\">def</span> verify<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1407</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-green-fg\">not</span> verify_nullability<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1408</span><span class=\"ansi-red-fg\">             </span>verify_value<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1409</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1410</span>     <span class=\"ansi-green-fg\">return</span> verify\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">verify_struct</span><span class=\"ansi-blue-fg\">(obj)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1385</span>                     raise ValueError(\n<span class=\"ansi-green-intense-fg ansi-bold\">   1386</span>                         new_msg(&#34;Length of object (%d) does not match with &#34;\n<span class=\"ansi-green-fg\">-&gt; 1387</span><span class=\"ansi-red-fg\">                                 &#34;length of fields (%d)&#34; % (len(obj), len(verifiers))))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1388</span>                 <span class=\"ansi-green-fg\">for</span> v<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>_<span class=\"ansi-blue-fg\">,</span> verifier<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">in</span> zip<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">,</span> verifiers<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1389</span>                     verifier<span class=\"ansi-blue-fg\">(</span>v<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ValueError</span>: Length of object (6) does not match with length of fields (7)</div>","errorSummary":"<span class=\"ansi-red-fg\">ValueError</span>: Length of object (6) does not match with length of fields (7)","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-400448375928439&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>         (99999, 10000, 1338.55, &#34;CA&#34;, &#34;insert&#34;, datetime.now())]  # record to insert\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> schema <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>table<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;loans_delta&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>schema\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>createOrReplaceTempView<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;merge_table&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> spark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;SELECT * FROM merge_table&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/databricks/utils/instrumentation.py</span> in <span class=\"ansi-cyan-fg\">wrapper</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     40</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span>             start_time <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 42</span><span class=\"ansi-red-fg\">             </span>return_val <span class=\"ansi-blue-fg\">=</span> func<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     43</span>         <span class=\"ansi-green-fg\">except</span> Exception<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     44</span>             duration <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span>time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-</span> start_time<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">1000</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    656</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-intense-fg ansi-bold\">    657</span>                 data, schema, samplingRatio, verifySchema)\n<span class=\"ansi-green-fg\">--&gt; 658</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    659</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    660</span>     <span class=\"ansi-green-fg\">def</span> _create_dataframe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    688</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    689</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 690</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    691</span>             jrdd <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>SerDeUtil<span class=\"ansi-blue-fg\">.</span>toJavaArray<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">.</span>_to_java_object_rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    692</span>             jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jsparkSession<span class=\"ansi-blue-fg\">.</span>applySchemaToPythonRDD<span class=\"ansi-blue-fg\">(</span>jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">.</span>json<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromLocal</span><span class=\"ansi-blue-fg\">(self, data, schema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    472</span>         write temp files<span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    473</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 474</span><span class=\"ansi-red-fg\">         </span>data<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_wrap_data_schema<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    475</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">.</span>parallelize<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema\n<span class=\"ansi-green-intense-fg ansi-bold\">    476</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_wrap_data_schema</span><span class=\"ansi-blue-fg\">(self, data, schema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    448</span>         <span class=\"ansi-red-fg\"># make sure data could consumed multiple times</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    449</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-green-fg\">not</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> list<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 450</span><span class=\"ansi-red-fg\">             </span>data <span class=\"ansi-blue-fg\">=</span> list<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    451</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    452</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">prepare</span><span class=\"ansi-blue-fg\">(obj)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    663</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    664</span>             <span class=\"ansi-green-fg\">def</span> prepare<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 665</span><span class=\"ansi-red-fg\">                 </span>verify_func<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    666</span>                 <span class=\"ansi-green-fg\">return</span> obj\n<span class=\"ansi-green-intense-fg ansi-bold\">    667</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">verify</span><span class=\"ansi-blue-fg\">(obj)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1406</span>     <span class=\"ansi-green-fg\">def</span> verify<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1407</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-green-fg\">not</span> verify_nullability<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1408</span><span class=\"ansi-red-fg\">             </span>verify_value<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1409</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1410</span>     <span class=\"ansi-green-fg\">return</span> verify\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">verify_struct</span><span class=\"ansi-blue-fg\">(obj)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1385</span>                     raise ValueError(\n<span class=\"ansi-green-intense-fg ansi-bold\">   1386</span>                         new_msg(&#34;Length of object (%d) does not match with &#34;\n<span class=\"ansi-green-fg\">-&gt; 1387</span><span class=\"ansi-red-fg\">                                 &#34;length of fields (%d)&#34; % (len(obj), len(verifiers))))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1388</span>                 <span class=\"ansi-green-fg\">for</span> v<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>_<span class=\"ansi-blue-fg\">,</span> verifier<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">in</span> zip<span class=\"ansi-blue-fg\">(</span>obj<span class=\"ansi-blue-fg\">,</span> verifiers<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1389</span>                     verifier<span class=\"ansi-blue-fg\">(</span>v<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ValueError</span>: Length of object (6) does not match with length of fields (7)</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%sql\nMERGE INTO loans_delta AS l\nUSING merge_table AS m\nON l.loan_id = m.loan_id\nWHEN MATCHED THEN \n  UPDATE SET *\nWHEN NOT MATCHED \n  THEN INSERT *;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18be935a-db63-49d6-a411-1d37fb072a28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Table or view not found: merge_table; line 1 pos 0;\n'MergeIntoTable ('l.loan_id = 'm.loan_id), [updateaction(None)], [insertaction(None)]\n:- SubqueryAlias l\n:  +- SubqueryAlias spark_catalog.deltadb.loans_delta\n:     +- Relation[loan_id#16205L,funded_amnt#16206,paid_amnt#16207,addr_state#16208,type#16209,timestamp#16210,credit_score#16211] parquet\n+- 'SubqueryAlias m\n   +- 'UnresolvedRelation [merge_table]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:109)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:95)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:192)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:191)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:191)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:191)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:191)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:191)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:191)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:180)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:200)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:91)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:171)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:81)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: AnalysisException: Table or view not found: merge_table; line 1 pos 0;\n'MergeIntoTable ('l.loan_id = 'm.loan_id), [updateaction(None)], [insertaction(None)]\n:- SubqueryAlias l\n:  +- SubqueryAlias spark_catalog.deltadb.loans_delta\n:     +- Relation[loan_id#16205L,funded_amnt#16206,paid_amnt#16207,addr_state#16208,type#16209,timestamp#16210,credit_score#16211] parquet\n+- 'SubqueryAlias m\n   +- 'UnresolvedRelation [merge_table]\n","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Table or view not found: merge_table; line 1 pos 0;\n'MergeIntoTable ('l.loan_id = 'm.loan_id), [updateaction(None)], [insertaction(None)]\n:- SubqueryAlias l\n:  +- SubqueryAlias spark_catalog.deltadb.loans_delta\n:     +- Relation[loan_id#16205L,funded_amnt#16206,paid_amnt#16207,addr_state#16208,type#16209,timestamp#16210,credit_score#16211] parquet\n+- 'SubqueryAlias m\n   +- 'UnresolvedRelation [merge_table]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:109)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:95)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:192)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:191)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:191)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:191)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:191)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:191)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:191)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:180)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:200)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:91)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:171)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:81)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:675)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:672)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:90)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:36)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:128)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:143)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$10(DriverLocal.scala:431)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:48)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:48)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:408)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:653)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:645)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:486)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:598)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:391)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["%sql SELECT * FROM loans_delta WHERE loan_id IN (4420, 99999)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d2f3ae7-d9f3-4f96-a896-4ecf61295cf0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[99999,5755,3905.461749111245,"WA","stream B","2021-12-09T15:43:24.328+0000",null],[99999,6768,5637.678147915814,"TX","stream A","2021-12-09T15:43:22.725+0000",null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"loan_id","type":"\"long\"","metadata":"{}"},{"name":"funded_amnt","type":"\"integer\"","metadata":"{}"},{"name":"paid_amnt","type":"\"double\"","metadata":"{}"},{"name":"addr_state","type":"\"string\"","metadata":"{}"},{"name":"type","type":"\"string\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"credit_score","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>loan_id</th><th>funded_amnt</th><th>paid_amnt</th><th>addr_state</th><th>type</th><th>timestamp</th><th>credit_score</th></tr></thead><tbody><tr><td>99999</td><td>5755</td><td>3905.461749111245</td><td>WA</td><td>stream B</td><td>2021-12-09T15:43:24.328+0000</td><td>null</td></tr><tr><td>99999</td><td>6768</td><td>5637.678147915814</td><td>TX</td><td>stream A</td><td>2021-12-09T15:43:22.725+0000</td><td>null</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## ![Delta Lake Logo Tiny](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) File compaction and performance optimizations = faster queries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16932b8a-32d8-4c1d-bae2-29b09b35c99e"}}},{"cell_type":"markdown","source":["### Vacuum"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad330e4f-6d36-445e-b457-afeb5c5a1619"}}},{"cell_type":"code","source":["%sql\n-- Vacuum deletes all files no longer needed by the current version of the table.\nVACUUM loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4540786-613d-4f01-9339-8c7270a1f900"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/user/hive/warehouse/deltadb.db/loans_delta"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/deltadb.db/loans_delta</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### <img src=\"https://pages.databricks.com/rs/094-YMS-629/images/dbsquare.png\" width=30/> Cache table in memory (Databricks Delta Lake only)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"816f7817-1997-4ad7-90bb-6264a3c1e558"}}},{"cell_type":"code","source":["%sql CACHE SELECT * FROM loans_delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23febc2e-e99b-4784-b685-d5256adb64ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### <img src=\"https://pages.databricks.com/rs/094-YMS-629/images/dbsquare.png\" width=30/> Z-Order Optimize (Databricks Delta Lake only)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca180ace-d2d7-48f1-9b75-b1bb19d27885"}}},{"cell_type":"code","source":["%sql OPTIMIZE loans_delta ZORDER BY addr_state"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e375c864-1d2d-4583-81de-b8de0536950c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[null,[1,43,[8264259,8264259,8264259.0,1,8264259],[789,1320218,217049.0,43,9333145],0,["minCubeSize(107374182400)",[0,0],[43,9333145],0,[43,9333145],1,null],1]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"metrics","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"numFilesAdded\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"filesAdded\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"filesRemoved\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"partitionsOptimized\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"zOrderStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"strategyName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"inputCubeFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputOtherFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputNumCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numOutputCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedNumCubes\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numBatches\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>metrics</th></tr></thead><tbody><tr><td>null</td><td>List(1, 43, List(8264259, 8264259, 8264259.0, 1, 8264259), List(789, 1320218, 217049.0, 43, 9333145), 0, List(minCubeSize(107374182400), List(0, 0), List(43, 9333145), 0, List(43, 9333145), 1, null), 1)</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["cleanup_paths_and_tables()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1eef2f5c-03fd-4ca0-84d7-4a834ec11475"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Delta Lake","dashboards":[{"elements":[{"elementNUID":"4dc4faec-0a21-4486-ab98-f06de53b90ba","guid":"aeaeac64-79fb-44b7-8ddd-d14aa23470e7","resultIndex":null,"options":null,"position":{"x":0,"y":0,"height":7,"width":8,"z":null},"elementType":"command"}],"guid":"0f591e66-f32a-412c-8f5e-a09354875601","layoutOption":{"stack":false,"grid":true},"version":"DashboardViewV1","nuid":"73b238d8-094e-4d18-a82a-1407ccc783bc","origId":400448375928452,"title":"Delta Lake Batch + Streaming","width":1600,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":400448375928372}},"nbformat":4,"nbformat_minor":0}
